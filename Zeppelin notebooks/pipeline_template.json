{"paragraphs":[{"text":"%md\n\n# PySpark Processing Pipeline Template\n\nThis notebook implements a typical pipeline for tabular data:\n- Split data into training and testing\n- Index all string columns (categorical columns) then perform one hot encoder. Missing is dealed with by handleInvalid='keep'\n- Impute all numeric columns, then standardize them\n- Assemble all processed columns in a Vector features\n\nUser parameters: \n- `hdfs_path`: path to HDFS folder\n- `data_file`: data file name\n- `split_ratio`: a list of two ratio, the training proportion and the testing proportion\n- `integer_cols`: a list of all integer columns' names. These will be casted to `double`\n- `drop_cols`: a list of all columns to drop from modeling data. These are usually ID columns or name columns\n- `string_cols`: a list of all string (categorical) columns. These will undergo one hot encoder\n- `numeric_cols`: a list of all numeric columns. These will undergo imputation and standardization\n- `target`: the single target column","user":"anonymous","dateUpdated":"2023-01-25T01:13:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>PySpark Processing Pipeline Template</h1>\n<p>This notebook implements a typical pipeline for tabular data:</p>\n<ul>\n<li>Split data into training and testing</li>\n<li>Index all string columns (categorical columns) then perform one hot encoder. Missing is dealed with by handleInvalid='keep'</li>\n<li>Impute all numeric columns, then standardize them</li>\n<li>Assemble all processed columns in a Vector features</li>\n</ul>\n<p>User parameters:</p>\n<ul>\n<li><code>hdfs_path</code>: path to HDFS folder</li>\n<li><code>data_file</code>: data file name</li>\n<li><code>split_ratio</code>: a list of two ratio, the training proportion and the testing proportion</li>\n<li><code>integer_cols</code>: a list of all integer columns' names. These will be casted to <code>double</code></li>\n<li><code>drop_cols</code>: a list of all columns to drop from modeling data. These are usually ID columns or name columns</li>\n<li><code>string_cols</code>: a list of all string (categorical) columns. These will undergo one hot encoder</li>\n<li><code>numeric_cols</code>: a list of all numeric columns. These will undergo imputation and standardization</li>\n<li><code>target</code>: the single target column</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1674568940156_1074212990","id":"20230124-200721_1933496500","dateCreated":"2023-01-24T14:02:20+0000","dateStarted":"2023-01-25T01:13:47+0000","dateFinished":"2023-01-25T01:13:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12670"},{"text":"%spark2.pyspark\n\n#path to data\nhdfs_path = '/tmp/data/'\ndata_file = 'heart_disease.csv'\nsplit_ratio = [0.7, 0.3]\ndrop_cols = ['PatientID']\ninteger_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'HeartDisease']\nstring_cols = ['ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\nnumeric_cols = ['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']\ntarget = 'HeartDisease'\n\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DoubleType\n\n#read data\ndata = spark.read.options(header='True',inferSchema='True',delimiter=',').csv(\"/tmp/data/heart_disease.csv\")\n\n#drop columns\ndata = data.drop(*drop_cols)\n\n#cast integer columns to double\nfor c in integer_cols:\n    data = data.withColumn(c, col(c).cast(DoubleType()))\n    \n#train-test split\ndata_train, data_test = data.randomSplit(split_ratio)\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer, StandardScaler, VectorAssembler\nfrom pyspark.ml import Pipeline\n\n###one hot encode the categorical columns\nencoders = []\nfor c in string_cols:\n    encoders.append(StringIndexer(inputCol=c, outputCol=c+'Index', handleInvalid='keep'))\n    encoders.append(OneHotEncoder(inputCol=c+'Index', outputCol=c+'Codes'))\n\n###impute the numeric columns\nimputer = Imputer(inputCols = numeric_cols, outputCols = [c+'Imp' for c in numeric_cols], strategy = 'median')\n\n###standardization\nnum_assembler = VectorAssembler(inputCols=[c+'Imp' for c in numeric_cols], outputCol='imputed')\nscaler = StandardScaler(inputCol = 'imputed', outputCol = 'scaled')\n\n###combine results\nassembler = VectorAssembler(inputCols=[c+'Codes' for c in string_cols]+['scaled'], outputCol='features')\n\n\n\n###build pipeline\npipeline = Pipeline(stages = encoders + [imputer, num_assembler, scaler, assembler])\n\n###train pipeline\npipeline_trained = pipeline.fit(data_train)","user":"anonymous","dateUpdated":"2023-01-25T01:21:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=0","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=1","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=2","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=3","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=4","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=5","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=6","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=7","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=8","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=9"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1674568940194_-514219966","id":"20230124-141029_1046446419","dateCreated":"2023-01-24T14:02:20+0000","dateStarted":"2023-01-25T01:21:44+0000","dateFinished":"2023-01-25T01:24:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12671"},{"text":"%md\n\n# Testing the pipeline\n\nPerform transformation on the training and testing set. This is a classification problem, so fit a Logistic model to demonstrate.\n","user":"anonymous","dateUpdated":"2023-01-25T01:13:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1674609145080_418729999","id":"20230125-011225_1765542937","dateCreated":"2023-01-25T01:12:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:14748","dateFinished":"2023-01-25T01:13:06+0000","dateStarted":"2023-01-25T01:13:06+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Testing the pipeline</h1>\n<p>Perform transformation on the training and testing set. This is a classification problem, so fit a Logistic model to demonstrate.</p>\n"}]}},{"text":"%spark2.pyspark\n\n#transform the training and testing data\ntrain_prc = pipeline_trained.transform(data_train).select(target,'features')\ntest_prc = pipeline_trained.transform(data_test).select(target,'features')\n\n#create and fit a logistic regression model\nfrom pyspark.ml.classification import LogisticRegression\nlogistic_model = LogisticRegression(featuresCol='features', labelCol=target)\nlogistic_trained = logistic_model.fit(train_prc)","user":"anonymous","dateUpdated":"2023-01-25T01:25:42+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=10","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=11","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=12","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=13","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=14","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=15","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=16","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=17","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=18","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=19","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=20","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=21","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=22","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=23","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=24","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=25","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=26","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=27","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=28","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=29","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=30","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=31","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=32","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=33","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=34","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=35","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=36","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=37","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=38","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=39","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=40","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=41","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=42","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=43","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=44","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=45","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=46","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=47","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=48","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=49","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=50","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=51","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=52","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=53","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=54","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=55","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=56","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=57","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=58","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=59","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=60","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=61","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=62","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=63","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=64","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=65","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=66"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1674568940197_-1861275465","id":"20230124-175256_1002768516","dateCreated":"2023-01-24T14:02:20+0000","dateStarted":"2023-01-25T01:25:43+0000","dateFinished":"2023-01-25T01:27:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12683"},{"text":"%spark2.pyspark\ntrain_predicted = logistic_trained.transform(train_prc)\ntrain_predicted.crosstab(target, 'prediction').show()","user":"anonymous","dateUpdated":"2023-01-25T01:25:46+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------------+---+---+\n|HeartDisease_prediction|0.0|1.0|\n+-----------------------+---+---+\n|                    1.0| 37|329|\n|                    0.0|246| 50|\n+-----------------------+---+---+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=67","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=68","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=69","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=70","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=71"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1674568940199_-1104188882","id":"20230124-184539_1694421190","dateCreated":"2023-01-24T14:02:20+0000","dateStarted":"2023-01-25T01:25:48+0000","dateFinished":"2023-01-25T01:27:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12688"},{"text":"%spark2.pyspark\ntest_predicted = logistic_trained.transform(test_prc)\ntest_predicted.crosstab(target, 'prediction').show()","user":"anonymous","dateUpdated":"2023-01-25T01:25:49+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------------+---+---+\n|HeartDisease_prediction|0.0|1.0|\n+-----------------------+---+---+\n|                    1.0| 15|127|\n|                    0.0| 91| 23|\n+-----------------------+---+---+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=72","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=73","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=74","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=75","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=76"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1674568940200_544336922","id":"20230124-184404_527645336","dateCreated":"2023-01-24T14:02:20+0000","dateStarted":"2023-01-25T01:27:00+0000","dateFinished":"2023-01-25T01:28:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12689"},{"text":"%spark2.pyspark\n","user":"anonymous","dateUpdated":"2023-01-25T00:52:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1674607921188_998014907","id":"20230125-005201_1902314815","dateCreated":"2023-01-25T00:52:01+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:12690"}],"name":"pipeline_template","id":"2HRN3KGVS","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}