{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11111111",
   "metadata": {},
   "source": [
    "# Classification template\n",
    "\n",
    "This notebook demonstrates the classification task in data analytics.\n",
    "\n",
    "In all examples, we will use the `heart_disease` dataset. The target is to predict whether a patient is having heart disease (`1`) or not (`0`) based on their other information.\n",
    "\n",
    "We will first load the data in and process using pipeline like in the `pipeline_template` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11111112",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "#path to data\n",
    "hdfs_path = '/tmp/data/'\n",
    "data_file = 'heart_disease.csv'\n",
    "split_ratio = [0.7, 0.3]\n",
    "drop_cols = ['PatientID']\n",
    "integer_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'HeartDisease']\n",
    "string_cols = ['ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "numeric_cols = ['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']\n",
    "target = 'HeartDisease'\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#read data\n",
    "data = spark.read.options(header='True',inferSchema='True',delimiter=',').csv(hdfs_path+data_file)\n",
    "\n",
    "#drop columns\n",
    "data = data.drop(*drop_cols)\n",
    "\n",
    "#cast integer columns to double\n",
    "for c in integer_cols:\n",
    "    data = data.withColumn(c, col(c).cast(DoubleType()))\n",
    "    \n",
    "#train-test split\n",
    "data_train, data_test = data.randomSplit(split_ratio)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "###one hot encode the categorical columns\n",
    "encoders = []\n",
    "for c in string_cols:\n",
    "    encoders.append(StringIndexer(inputCol=c, outputCol=c+'Index', handleInvalid='keep'))\n",
    "    encoders.append(OneHotEncoder(inputCol=c+'Index', outputCol=c+'Codes'))\n",
    "\n",
    "###impute the numeric columns\n",
    "imputer = Imputer(inputCols = numeric_cols, outputCols = [c+'Imp' for c in numeric_cols], strategy = 'median')\n",
    "\n",
    "###standardization\n",
    "num_assembler = VectorAssembler(inputCols=[c+'Imp' for c in numeric_cols], outputCol='imputed')\n",
    "scaler = StandardScaler(inputCol = 'imputed', outputCol = 'scaled')\n",
    "\n",
    "###combine results\n",
    "assembler = VectorAssembler(inputCols=[c+'Codes' for c in string_cols]+['scaled'], outputCol='features')\n",
    "\n",
    "\n",
    "\n",
    "###build pipeline\n",
    "pipeline = Pipeline(stages = encoders + [imputer, num_assembler, scaler, assembler])\n",
    "\n",
    "###train pipeline\n",
    "pipeline_trained = pipeline.fit(data_train)\n",
    "\n",
    "###process training data annd testing data\n",
    "train_prc = pipeline_trained.transform(data_train).select(target,'features')\n",
    "test_prc = pipeline_trained.transform(data_test).select(target,'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111113",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We will tune and test some common classification models:\n",
    "- Logistic regression\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- Gradient boosting model\n",
    "- Multilayer Perceptron (Neural networks)\n",
    "\n",
    "We can automate the search for the best hyperparamters with Cross Validation Grid Search. In pyspark, we use a combination of `ParamGridBuilder` and `CrossValidator`. The steps are as follows\n",
    "1. Create an empty model\n",
    "2. Create the parameter grid with `ParamGridBuilder`. Each hyperparameter requires a different `addGrid()` call; multiple `addGrid()` can be chained.\n",
    "3. Create the `CrossValidator` object\n",
    "    - `estimator`: the empty model\n",
    "    - `estimatorParamMaps`: the parameter grid\n",
    "    - `evaluator`: the evaluator object (`MulticlassClassificationEvaluator` for classification)\n",
    "    - `numFolds`: number of folds for cross validation\n",
    "4. Train the CrossValidator with fit()\n",
    "\n",
    "First, we import general libaries and create an evaluator. `metricName` are commonly `f1` or `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11111114",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target, metricName='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111115",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Logistic regression has one hyperparameters, `regParam` and `elasticNetParam`, to tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11111116",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation logistic regression\n",
       "('training F1: ', 0.8529777439466435)\n",
       "('testing F1: ', 0.872577764672501)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#create empty model\n",
    "logistic = LogisticRegression(featuresCol='features', labelCol=target)\n",
    "\n",
    "#parameter grid for decision tree\n",
    "paramGridLogistic = ParamGridBuilder().addGrid(logistic.regParam, [0.001, 0.01, 0.1, 1., 10.])\\\n",
    "                                      .addGrid(logistic.elasticNetParam, [0.25, 0.5, 0.75]).build()\n",
    "\n",
    "#cross validator\n",
    "crossval = CrossValidator(estimator=logistic,\n",
    "                          estimatorParamMaps=paramGridLogistic,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3) \n",
    "\n",
    "#perform the search\n",
    "cvLogistic = crossval.fit(train_prc)\n",
    "\n",
    "#test the tuned model\n",
    "train_pred_cvLogistic = cvLogistic.transform(train_prc)\n",
    "test_pred_cvLogistic = cvLogistic.transform(test_prc)\n",
    "print('cross-validation logistic regression')\n",
    "print('training F1: ', evaluator.evaluate(train_pred_cvLogistic))\n",
    "print('testing F1: ', evaluator.evaluate(test_pred_cvLogistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111117",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "The two important hyperparameters to tune for decision tree are `maxDepth` and `minInstancesPerNode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11111118",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation decision tree\n",
       "('training F1: ', 0.8580293817175548)\n",
       "('testing F1: ', 0.8310770136876168)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "#create empty model\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol=target)\n",
    "\n",
    "#parameter grid for decision tree\n",
    "paramGridTree = ParamGridBuilder().addGrid(dt.maxDepth, [3, 5, 7])\\\n",
    "                                  .addGrid(dt.minInstancesPerNode, [10, 20, 30]).build()\n",
    "\n",
    "#cross validator\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGridTree,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3) \n",
    "\n",
    "#perform the search\n",
    "cvTree = crossval.fit(train_prc)\n",
    "\n",
    "#test the tuned model\n",
    "train_pred_cvTree = cvTree.transform(train_prc)\n",
    "test_pred_cvTree = cvTree.transform(test_prc)\n",
    "print('cross-validation decision tree')\n",
    "print('training F1: ', evaluator.evaluate(train_pred_cvTree))\n",
    "print('testing F1: ', evaluator.evaluate(test_pred_cvTree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111119",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble of decision trees and usually yields better performances. \n",
    "\n",
    "Similar to a tree, we need to tune maxDepth and `minInstancesPerNode`. We also need to tune `numTrees` - the number of trees in a forest model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11111120",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation random forest\n",
       "('training F1: ', 0.9038743620786964)\n",
       "('testing F1: ', 0.8409396703765732)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#initialize model\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol=target)\n",
    "\n",
    "#paramter grid\n",
    "paramGridForest = ParamGridBuilder().addGrid(rf.numTrees, [10, 30, 50])\\\n",
    "                                    .addGrid(dt.maxDepth, [3, 5, 7])\\\n",
    "                                    .addGrid(dt.minInstancesPerNode, [10, 20, 30])\\\n",
    "                                    .build()\n",
    "#cross validator\n",
    "crossval = CrossValidator(estimator = rf,\n",
    "                          estimatorParamMaps = paramGridForest,\n",
    "                          evaluator = evaluator,\n",
    "                          numFolds = 3) \n",
    "\n",
    "#perform tuning\n",
    "cvForest = crossval.fit(train_prc)\n",
    "\n",
    "#test the tuned model\n",
    "train_pred_cvForest = cvForest.transform(train_prc)\n",
    "test_pred_cvForest = cvForest.transform(test_prc)\n",
    "\n",
    "print('cross-validation random forest')\n",
    "print('training F1: ', evaluator.evaluate(train_pred_cvForest))\n",
    "print('testing F1: ', evaluator.evaluate(test_pred_cvForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111121",
   "metadata": {},
   "source": [
    "<h4>Gradient Boosting Model</h4>\n",
    "\n",
    "Gradient boosting model (GBT) is similar to random forest, however, each tree is added to the ensemble to minimize the current training error instead of randomly.\n",
    "\n",
    "GBT models still have `maxDepth` and `minInstancesPerNode` to tune, however, we do not tune the numTrees anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11111122",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation GBT\n",
       "('training F1: ', 0.8751703579201073)\n",
       "('testing F1: ', 0.8442918343712799)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol=target)\n",
    "\n",
    "paramGridGBT = ParamGridBuilder().addGrid(gbt.maxDepth, [3, 5, 7])\\\n",
    "                                 .addGrid(gbt.minInstancesPerNode, [10, 20, 30])\\\n",
    "                                 .build()\n",
    "\n",
    "crossval = CrossValidator(estimator = gbt,\n",
    "                          estimatorParamMaps = paramGridGBT,\n",
    "                          evaluator = evaluator,\n",
    "                          numFolds = 3) \n",
    "\n",
    "cvGBT = crossval.fit(train_prc)\n",
    "\n",
    "train_pred_cvGBT = cvGBT.transform(train_prc)\n",
    "test_pred_cvGBT = cvGBT.transform(test_prc)\n",
    "\n",
    "print('cross-validation GBT')\n",
    "print('training F1: ', evaluator.evaluate(train_pred_cvGBT))\n",
    "print('testing F1: ', evaluator.evaluate(test_pred_cvGBT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111123",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "\n",
    "Pyspark's version of neural networks. Only has sigmoid activations. We need to tune the `layers` hyperparameter which is a list of neurons per layer and must include the sizes of the input and output layers.\n",
    "\n",
    "The size of the input layer can be observed with `train_prc.head()` and the size of the output layer is the number of unique classes in the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11111124",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(HeartDisease=0.0, features=SparseVector(18, {2: 1.0, 5: 1.0, 7: 1.0, 10: 1.0, 12: 3.1433, 13: 6.9639, 14: 1.8085, 16: 7.979}))]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "train_prc.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11111125",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+----------------------------+\n",
       "|count(DISTINCT HeartDisease)|\n",
       "+----------------------------+\n",
       "|                           2|\n",
       "+----------------------------+\n",
       "\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "train_prc.select(countDistinct(target)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11111126",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation MLP\n",
       "('training F1: ', 0.8654006062912944)\n",
       "('testing F1: ', 0.8309315136522067)\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol='features', labelCol=target)\n",
    "\n",
    "paramGridMLP = ParamGridBuilder().addGrid(mlp.layers, [\n",
    "        [18, 20, 2],\n",
    "        [18, 20, 20, 2],\n",
    "        [18, 30, 2],\n",
    "        [18, 30, 30, 2]])\\\n",
    "        .addGrid(mlp.maxIter, [100,200,300]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator = mlp,\n",
    "                          estimatorParamMaps = paramGridMLP,\n",
    "                          evaluator = evaluator,\n",
    "                          numFolds = 3) \n",
    "\n",
    "cvMLP = crossval.fit(train_prc)\n",
    "\n",
    "train_pred_cvMLP = cvMLP.transform(train_prc)\n",
    "test_pred_cvMLP = cvMLP.transform(test_prc)\n",
    "\n",
    "print('cross-validation MLP')\n",
    "print('training F1: ', evaluator.evaluate(train_pred_cvMLP))\n",
    "print('testing F1: ', evaluator.evaluate(test_pred_cvMLP))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
