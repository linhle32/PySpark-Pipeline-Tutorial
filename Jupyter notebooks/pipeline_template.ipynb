{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11111111",
   "metadata": {},
   "source": [
    "# PySpark Processing Pipeline Template\n",
    "\n",
    "This notebook implements a typical pipeline for tabular data:\n",
    "- Split data into training and testing\n",
    "- Index all string columns (categorical columns) then perform one hot encoder. Missing is dealed with by handleInvalid='keep'\n",
    "- Impute all numeric columns, then standardize them\n",
    "- Assemble all processed columns in a Vector features\n",
    "\n",
    "User parameters: \n",
    "- `hdfs_path`: path to HDFS folder\n",
    "- `data_file`: data file name\n",
    "- `split_ratio`: a list of two ratio, the training proportion and the testing proportion\n",
    "- `integer_cols`: a list of all integer columns' names. These will be casted to `double`\n",
    "- `drop_cols`: a list of all columns to drop from modeling data. These are usually ID columns or name columns\n",
    "- `string_cols`: a list of all string (categorical) columns. These will undergo one hot encoder\n",
    "- `numeric_cols`: a list of all numeric columns. These will undergo imputation and standardization\n",
    "- `target`: the single target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11111112",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "#path to data\n",
    "hdfs_path = '/tmp/data/'\n",
    "data_file = 'heart_disease.csv'\n",
    "split_ratio = [0.7, 0.3]\n",
    "drop_cols = ['PatientID']\n",
    "integer_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'HeartDisease']\n",
    "string_cols = ['ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "numeric_cols = ['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']\n",
    "target = 'HeartDisease'\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#read data\n",
    "data = spark.read.options(header='True',inferSchema='True',delimiter=',').csv(\"/tmp/data/heart_disease.csv\")\n",
    "\n",
    "#drop columns\n",
    "data = data.drop(*drop_cols)\n",
    "\n",
    "#cast integer columns to double\n",
    "for c in integer_cols:\n",
    "    data = data.withColumn(c, col(c).cast(DoubleType()))\n",
    "    \n",
    "#train-test split\n",
    "data_train, data_test = data.randomSplit(split_ratio)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "###one hot encode the categorical columns\n",
    "encoders = []\n",
    "for c in string_cols:\n",
    "    encoders.append(StringIndexer(inputCol=c, outputCol=c+'Index', handleInvalid='keep'))\n",
    "    encoders.append(OneHotEncoder(inputCol=c+'Index', outputCol=c+'Codes'))\n",
    "\n",
    "###impute the numeric columns\n",
    "imputer = Imputer(inputCols = numeric_cols, outputCols = [c+'Imp' for c in numeric_cols], strategy = 'median')\n",
    "\n",
    "###standardization\n",
    "num_assembler = VectorAssembler(inputCols=[c+'Imp' for c in numeric_cols], outputCol='imputed')\n",
    "scaler = StandardScaler(inputCol = 'imputed', outputCol = 'scaled')\n",
    "\n",
    "###combine results\n",
    "assembler = VectorAssembler(inputCols=[c+'Codes' for c in string_cols]+['scaled'], outputCol='features')\n",
    "\n",
    "\n",
    "\n",
    "###build pipeline\n",
    "pipeline = Pipeline(stages = encoders + [imputer, num_assembler, scaler, assembler])\n",
    "\n",
    "###train pipeline\n",
    "pipeline_trained = pipeline.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111113",
   "metadata": {},
   "source": [
    "# Testing the pipeline\n",
    "\n",
    "Perform transformation on the training and testing set. This is a classification problem, so fit a Logistic model to demonstrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11111114",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "#transform the training and testing data\n",
    "train_prc = pipeline_trained.transform(data_train).select(target,'features')\n",
    "test_prc = pipeline_trained.transform(data_test).select(target,'features')\n",
    "\n",
    "#create and fit a logistic regression model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "logistic_model = LogisticRegression(featuresCol='features', labelCol=target)\n",
    "logistic_trained = logistic_model.fit(train_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11111115",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+---+---+\n",
       "|HeartDisease_prediction|0.0|1.0|\n",
       "+-----------------------+---+---+\n",
       "|                    1.0| 37|329|\n",
       "|                    0.0|246| 50|\n",
       "+-----------------------+---+---+\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "train_predicted = logistic_trained.transform(train_prc)\n",
    "train_predicted.crosstab(target, 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11111116",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+---+---+\n",
       "|HeartDisease_prediction|0.0|1.0|\n",
       "+-----------------------+---+---+\n",
       "|                    1.0| 15|127|\n",
       "|                    0.0| 91| 23|\n",
       "+-----------------------+---+---+\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "test_predicted = logistic_trained.transform(test_prc)\n",
    "test_predicted.crosstab(target, 'prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
