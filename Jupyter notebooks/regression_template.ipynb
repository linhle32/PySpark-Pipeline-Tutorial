{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11111111",
   "metadata": {},
   "source": [
    "<h3> Regression </h3>\n",
    "\n",
    "This notebook demonstrates the regression task in data analytics.\n",
    "\n",
    "In all examples, we will use the `students_reg` dataset. The data was collected from students at week 12 of their 2nd freshman semester. The target is to predict their final first year GPA based on their other information.\n",
    "\n",
    "We will first load the data in and process using pipeline like in the previous module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11111112",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "#path to data\n",
    "hdfs_path = '/tmp/data/'\n",
    "data_file = 'students_reg.csv'\n",
    "split_ratio = [0.7, 0.3]\n",
    "drop_cols = ['StudentID', 'FirstName', 'LastName']\n",
    "integer_cols = ['FamilyIncome', 'TotalAbsence']\n",
    "string_cols = ['Major', 'State']\n",
    "numeric_cols = ['HighSchoolGPA','FamilyIncome','AvgDailyStudyTime','TotalAbsence']\n",
    "target = 'FirstYearGPA'\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#read data\n",
    "data = spark.read.options(header='True',inferSchema='True',delimiter=',').csv(hdfs_path+data_file)\n",
    "\n",
    "#drop columns\n",
    "data = data.drop(*drop_cols)\n",
    "\n",
    "#cast integer columns to double\n",
    "for c in integer_cols:\n",
    "    data = data.withColumn(c, col(c).cast(DoubleType()))\n",
    "    \n",
    "#train-test split\n",
    "data_train, data_test = data.randomSplit(split_ratio)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "###one hot encode the categorical columns\n",
    "encoders = []\n",
    "for c in string_cols:\n",
    "    encoders.append(StringIndexer(inputCol=c, outputCol=c+'Index', handleInvalid='keep'))\n",
    "    encoders.append(OneHotEncoder(inputCol=c+'Index', outputCol=c+'Codes'))\n",
    "\n",
    "###impute the numeric columns\n",
    "imputer = Imputer(inputCols = numeric_cols, outputCols = [c+'Imp' for c in numeric_cols], strategy = 'median')\n",
    "\n",
    "###standardization\n",
    "num_assembler = VectorAssembler(inputCols=[c+'Imp' for c in numeric_cols], outputCol='imputed')\n",
    "scaler = StandardScaler(inputCol = 'imputed', outputCol = 'scaled')\n",
    "\n",
    "###combine results\n",
    "assembler = VectorAssembler(inputCols=[c+'Codes' for c in string_cols]+['scaled'], outputCol='features')\n",
    "\n",
    "\n",
    "\n",
    "###build pipeline\n",
    "pipeline = Pipeline(stages = encoders + [imputer, num_assembler, scaler, assembler])\n",
    "\n",
    "###train pipeline\n",
    "pipeline_trained = pipeline.fit(data_train)\n",
    "\n",
    "###process training data annd testing data\n",
    "train_prc = pipeline_trained.transform(data_train).select(target,'features')\n",
    "test_prc = pipeline_trained.transform(data_test).select(target,'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111113",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We will tune and test some common regression models available in PySpark:\n",
    "- Linear regression\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- Gradient boosting model\n",
    "\n",
    "We can automate the search for the best hyperparamters with Cross Validation Grid Search. In pyspark, we use a combination of `ParamGridBuilder` and `CrossValidator`. The steps are as follows\n",
    "1. Create an empty model\n",
    "2. Create the parameter grid with `ParamGridBuilder`. Each hyperparameter requires a different `addGrid()` call; multiple `addGrid()` can be chained.\n",
    "3. Create the `CrossValidator` object\n",
    "    - `estimator`: the empty model\n",
    "    - `estimatorParamMaps`: the parameter grid\n",
    "    - `evaluator`: the evaluator object (`RegressionEvaluator` for regression)\n",
    "    - `numFolds`: number of folds for cross validation\n",
    "4. Train the CrossValidator with fit()\n",
    "\n",
    "First, we import general libaries and create an evaluator. `metricName` are commonly `mse` or `r2`. Lower MSE and higher R2 mean better models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11111114",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "eval_mse = RegressionEvaluator(labelCol=target, metricName='mse')\n",
    "eval_r2 = RegressionEvaluator(labelCol=target, metricName='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111115",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Linear regression has two hyperparameters, `regParam` and `elasticNetParam`, to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11111116",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation linear regression\n",
       "('training MSE: ', 0.04674396580841727)\n",
       "('training R2: ', 0.8446809698340348)\n",
       "('testing MSE: ', 0.04695639677048426)\n",
       "('testing R2: ', 0.8484984403489446)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol=target)\n",
    "\n",
    "paramGridLR = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 1.0, 10.0])\\\n",
    "                                .addGrid(lr.elasticNetParam, [0.25, 0.5, 0.75])\\\n",
    "                                .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGridLR,\n",
    "                          evaluator=eval_r2,\n",
    "                          numFolds=10) \n",
    "\n",
    "cvLR = crossval.fit(train_prc)\n",
    "\n",
    "train_pred_cvLR = cvLR.transform(train_prc)\n",
    "test_pred_cvLR = cvLR.transform(test_prc)\n",
    "\n",
    "print('cross-validation linear regression')\n",
    "print('training MSE: ', eval_mse.evaluate(train_pred_cvLR))\n",
    "print('training R2: ', eval_r2.evaluate(train_pred_cvLR))\n",
    "print('testing MSE: ', eval_mse.evaluate(test_pred_cvLR))\n",
    "print('testing R2: ', eval_r2.evaluate(test_pred_cvLR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111117",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "The two important hyperparameters to tune for decision tree are `maxDepth` and `minInstancesPerNode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11111118",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation decision tree\n",
       "('training MSE: ', 0.04290978170608536)\n",
       "('training R2: ', 0.8574210475307527)\n",
       "('testing MSE: ', 0.07325860074639325)\n",
       "('testing R2: ', 0.7636362022158183)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "#create empty model\n",
    "dt = DecisionTreeRegressor(featuresCol='features', labelCol=target)\n",
    "\n",
    "#parameter grid for decision tree\n",
    "paramGridTree = ParamGridBuilder().addGrid(dt.maxDepth, [3, 5, 7])\\\n",
    "                                  .addGrid(dt.minInstancesPerNode, [10, 20, 30]).build()\n",
    "\n",
    "#cross validator\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGridTree,\n",
    "                          evaluator=eval_r2,\n",
    "                          numFolds=3) \n",
    "\n",
    "#perform the search\n",
    "cvTree = crossval.fit(train_prc)\n",
    "\n",
    "#test the tuned model\n",
    "train_pred_cvTree = cvTree.transform(train_prc)\n",
    "test_pred_cvTree = cvTree.transform(test_prc)\n",
    "\n",
    "print('cross-validation decision tree')\n",
    "print('training MSE: ', eval_mse.evaluate(train_pred_cvTree))\n",
    "print('training R2: ', eval_r2.evaluate(train_pred_cvTree))\n",
    "print('testing MSE: ', eval_mse.evaluate(test_pred_cvTree))\n",
    "print('testing R2: ', eval_r2.evaluate(test_pred_cvTree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111119",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble of decision trees and usually yields better performances. \n",
    "\n",
    "Similar to a tree, we need to tune maxDepth and `minInstancesPerNode`. We also need to tune `numTrees` - the number of trees in a forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11111120",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation random forest\n",
       "('training MSE: ', 0.06105781817749406)\n",
       "('training R2: ', 0.7971194583222437)\n",
       "('testing MSE: ', 0.08102633499630833)\n",
       "('testing R2: ', 0.7385741460369937)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "#initialize model\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol=target)\n",
    "\n",
    "#paramter grid\n",
    "paramGridForest = ParamGridBuilder().addGrid(rf.numTrees, [10, 30, 50])\\\n",
    "                                    .addGrid(dt.maxDepth, [3, 5, 7])\\\n",
    "                                    .addGrid(dt.minInstancesPerNode, [10, 20, 30])\\\n",
    "                                    .build()\n",
    "#cross validator\n",
    "crossval = CrossValidator(estimator = rf,\n",
    "                          estimatorParamMaps = paramGridForest,\n",
    "                          evaluator = eval_r2,\n",
    "                          numFolds = 3) \n",
    "\n",
    "#perform tuning\n",
    "cvForest = crossval.fit(train_prc)\n",
    "\n",
    "#test the tuned model\n",
    "train_pred_cvForest = cvForest.transform(train_prc)\n",
    "test_pred_cvForest = cvForest.transform(test_prc)\n",
    "\n",
    "print('cross-validation random forest')\n",
    "print('training MSE: ', eval_mse.evaluate(train_pred_cvForest))\n",
    "print('training R2: ', eval_r2.evaluate(train_pred_cvForest))\n",
    "print('testing MSE: ', eval_mse.evaluate(test_pred_cvForest))\n",
    "print('testing R2: ', eval_r2.evaluate(test_pred_cvForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111121",
   "metadata": {},
   "source": [
    "<h4>Gradient Boosting Model</h4>\n",
    "\n",
    "Gradient boosting model (GBT) is similar to random forest, however, each tree is added to the ensemble to minimize the current training error instead of randomly.\n",
    "\n",
    "GBT models still have `maxDepth` and minInstancesPerNode to tune, however, we do not tune the `numTrees` anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11111122",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross-validation random forest\n",
       "('training MSE: ', 0.044394343701222395)\n",
       "('training R2: ', 0.8524882027171358)\n",
       "('testing MSE: ', 0.06115345735534525)\n",
       "('testing R2: ', 0.802692608364431)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "gbt = GBTRegressor(featuresCol='features', labelCol=target)\n",
    "\n",
    "paramGridGBT = ParamGridBuilder().addGrid(gbt.maxDepth, [3, 5, 7])\\\n",
    "                                 .addGrid(gbt.minInstancesPerNode, [10, 20, 30])\\\n",
    "                                 .build()\n",
    "\n",
    "crossval = CrossValidator(estimator = gbt,\n",
    "                          estimatorParamMaps = paramGridGBT,\n",
    "                          evaluator = eval_r2,\n",
    "                          numFolds = 3) \n",
    "\n",
    "cvGBT = crossval.fit(train_prc)\n",
    "\n",
    "train_pred_cvGBT = cvGBT.transform(train_prc)\n",
    "test_pred_cvGBT = cvGBT.transform(test_prc)\n",
    "\n",
    "print('cross-validation random forest')\n",
    "print('training MSE: ', eval_mse.evaluate(train_pred_cvGBT))\n",
    "print('training R2: ', eval_r2.evaluate(train_pred_cvGBT))\n",
    "print('testing MSE: ', eval_mse.evaluate(test_pred_cvGBT))\n",
    "print('testing R2: ', eval_r2.evaluate(test_pred_cvGBT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
