{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11111111",
   "metadata": {},
   "source": [
    "# PySpark Processing Pipeline \n",
    "\n",
    "This notebook aims to explain major steps in a data processing pipeline. Therefore, most the steps come with some code to check their results (mostly `printSchema()` and `show()`. For an end-to-end pipeline, please use the `pipeline_template` notebook.\n",
    "\n",
    "### Loading Data\n",
    "\n",
    "Change the csv path to your correct file which should be stored in a HDFS cluster.\n",
    "\n",
    "`printSchema()` will display the columns' names and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11111112",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- Age: integer (nullable = true)\n",
       " |-- Sex: string (nullable = true)\n",
       " |-- ChestPainType: string (nullable = true)\n",
       " |-- RestingBP: integer (nullable = true)\n",
       " |-- Cholesterol: integer (nullable = true)\n",
       " |-- FastingBS: integer (nullable = true)\n",
       " |-- RestingECG: string (nullable = true)\n",
       " |-- MaxHR: integer (nullable = true)\n",
       " |-- ExerciseAngina: string (nullable = true)\n",
       " |-- Oldpeak: double (nullable = true)\n",
       " |-- ST_Slope: string (nullable = true)\n",
       " |-- HeartDisease: integer (nullable = true)\n",
       "\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "#path to data\n",
    "hdfs_path = '/tmp/data/'\n",
    "data_file = 'heart_disease.csv'\n",
    "\n",
    "data = spark.read.options(header='True',inferSchema='True',delimiter=',').csv(\"/tmp/data/heart_disease.csv\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111113",
   "metadata": {},
   "source": [
    "Check some rows with `show()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11111114",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
       "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
       "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
       "| 40|  M|          ATA|      140|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
       "| 49|  F|          NAP|      160|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|\n",
       "| 37|  M|          ATA|      130|        283|        0|        ST|   98|             N|    0.0|      Up|           0|\n",
       "| 48|  F|          ASY|      138|        214|        0|    Normal|  108|             Y|    1.5|    Flat|           1|\n",
       "| 54|  M|          NAP|      150|        195|        0|    Normal|  122|             N|    0.0|      Up|           0|\n",
       "| 39|  M|          NAP|      120|        339|        0|    Normal|  170|             N|    0.0|      Up|           0|\n",
       "| 45|  F|          ATA|      130|        237|        0|    Normal|  170|             N|    0.0|      Up|           0|\n",
       "| 54|  M|          ATA|      110|        208|        0|    Normal|  142|             N|    0.0|      Up|           0|\n",
       "| 37|  M|          ASY|      140|        207|        0|    Normal|  130|             Y|    1.5|    Flat|           1|\n",
       "| 48|  F|          ATA|      120|        284|        0|    Normal|  120|             N|    0.0|      Up|           0|\n",
       "| 37|  F|          NAP|      130|        211|        0|    Normal|  142|             N|    0.0|      Up|           0|\n",
       "| 58|  M|          ATA|      136|        164|        0|        ST|   99|             Y|    2.0|    Flat|           1|\n",
       "| 39|  M|          ATA|      120|        204|        0|    Normal|  145|             N|    0.0|      Up|           0|\n",
       "| 49|  M|          ASY|      140|        234|        0|    Normal|  140|             Y|    1.0|    Flat|           1|\n",
       "| 42|  F|          NAP|      115|        211|        0|        ST|  137|             N|    0.0|      Up|           0|\n",
       "| 54|  F|          ATA|      120|        273|        0|    Normal|  150|             N|    1.5|    Flat|           0|\n",
       "| 38|  M|          ASY|      110|        196|        0|    Normal|  166|             N|    0.0|    Flat|           1|\n",
       "| 43|  F|          ATA|      120|        201|        0|    Normal|  165|             N|    0.0|      Up|           0|\n",
       "| 60|  M|          ASY|      100|        248|        0|    Normal|  125|             N|    1.0|    Flat|           1|\n",
       "| 36|  M|          ATA|      120|        267|        0|    Normal|  160|             N|    3.0|    Flat|           1|\n",
       "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111115",
   "metadata": {},
   "source": [
    "#### Change all integer columns to float\n",
    "\n",
    "We need to change all integer columns to float types, otherwise, we will get errors in modeling.\n",
    "\n",
    "Modify the code below to include all integer columns in `integer_cols` (as strings). `printSchema()` then verifies if everything is double. In my example, `PatientID` is not casted since we will drop it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11111116",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- Age: double (nullable = true)\n",
       " |-- Sex: string (nullable = true)\n",
       " |-- ChestPainType: string (nullable = true)\n",
       " |-- RestingBP: double (nullable = true)\n",
       " |-- Cholesterol: double (nullable = true)\n",
       " |-- FastingBS: double (nullable = true)\n",
       " |-- RestingECG: string (nullable = true)\n",
       " |-- MaxHR: double (nullable = true)\n",
       " |-- ExerciseAngina: string (nullable = true)\n",
       " |-- Oldpeak: double (nullable = true)\n",
       " |-- ST_Slope: string (nullable = true)\n",
       " |-- HeartDisease: double (nullable = true)\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "integer_cols = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'HeartDisease']\n",
    "\n",
    "for c in integer_cols:\n",
    "    data = data.withColumn(c, col(c).cast(DoubleType()))\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111117",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns\n",
    "\n",
    "Drop all unneccessary columns in the paragraph below by including them in the `drop_cols` list. Verify with the result of `printSchema()`. \n",
    "\n",
    "In general, ID columns and name columns (first name, last name, middle name, etc) should be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11111118",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- Age: double (nullable = true)\n",
       " |-- Sex: string (nullable = true)\n",
       " |-- ChestPainType: string (nullable = true)\n",
       " |-- RestingBP: double (nullable = true)\n",
       " |-- Cholesterol: double (nullable = true)\n",
       " |-- FastingBS: double (nullable = true)\n",
       " |-- RestingECG: string (nullable = true)\n",
       " |-- MaxHR: double (nullable = true)\n",
       " |-- ExerciseAngina: string (nullable = true)\n",
       " |-- Oldpeak: double (nullable = true)\n",
       " |-- ST_Slope: string (nullable = true)\n",
       " |-- HeartDisease: double (nullable = true)\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "drop_cols = ['PatientID']\n",
    "data_main = data.drop(*drop_cols)\n",
    "data_main.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111119",
   "metadata": {},
   "source": [
    "### Train Test Splitting\n",
    "\n",
    "Change the 0.7 - 0.3 ratio to other as needed. Then, we use `count()` to verify the sizes of the two sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11111120",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651, 267)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "data_train, data_test = data_main.randomSplit([0.7, 0.3])\n",
    "\n",
    "data_train.count(), data_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111121",
   "metadata": {},
   "source": [
    "### Processing Pipeline\n",
    "\n",
    "Modify `string_cols`, `numeric_cols`, and `target` to include the correct columns in each list. The pipeline below will\n",
    "- Index all string columns (categorical columns) then perform one hot encoder. Missing is dealed with by `handleInvalid='keep'`\n",
    "- Impute all numeric columns, then standardize them\n",
    "- Assemble all processed columns in a Vector `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11111122",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "string_cols = ['ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "numeric_cols = ['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']\n",
    "target = 'HeartDisease'\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "###one hot encode the categorical columns\n",
    "encoders = []\n",
    "for c in string_cols:\n",
    "    encoders.append(StringIndexer(inputCol=c, outputCol=c+'Index', handleInvalid='keep'))\n",
    "    encoders.append(OneHotEncoder(inputCol=c+'Index', outputCol=c+'Codes'))\n",
    "\n",
    "###impute the numeric columns\n",
    "imputer = Imputer(inputCols = numeric_cols, outputCols = [c+'Imp' for c in numeric_cols], strategy = 'median')\n",
    "\n",
    "###standardization\n",
    "num_assembler = VectorAssembler(inputCols=[c+'Imp' for c in numeric_cols], outputCol='imputed')\n",
    "scaler = StandardScaler(inputCol = 'imputed', outputCol = 'scaled')\n",
    "\n",
    "###combine results\n",
    "assembler = VectorAssembler(inputCols=[c+'Codes' for c in string_cols]+['scaled'], outputCol='features')\n",
    "\n",
    "\n",
    "\n",
    "###build pipeline\n",
    "pipeline = Pipeline(stages = encoders + [imputer, num_assembler, scaler, assembler])\n",
    "\n",
    "###train pipeline\n",
    "pipeline_trained = pipeline.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111123",
   "metadata": {},
   "source": [
    "#### Transform the training data with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11111124",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+------------+--------------------+\n",
       "|HeartDisease|            features|\n",
       "+------------+--------------------+\n",
       "|         0.0|(18,[2,6,7,10,12,...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[2,6,7,10,12,...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[3,5,7,10,12,...|\n",
       "|         0.0|(18,[2,5,7,10,12,...|\n",
       "|         1.0|(18,[0,4,8,9,12,1...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         1.0|(18,[0,4,7,9,12,1...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         1.0|(18,[3,4,7,10,12,...|\n",
       "|         1.0|(18,[0,4,8,9,12,1...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         1.0|(18,[0,4,7,10,12,...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[2,5,7,10,12,...|\n",
       "|         1.0|(18,[3,4,7,9,12,1...|\n",
       "|         0.0|(18,[0,4,7,10,12,...|\n",
       "|         0.0|(18,[3,5,7,10,12,...|\n",
       "|         1.0|(18,[0,4,8,9,12,1...|\n",
       "+------------+--------------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "train_prc = pipeline_trained.transform(data_train).select(target,'features')\n",
    "train_prc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111125",
   "metadata": {},
   "source": [
    "#### Transform the testing data with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11111126",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+------------+--------------------+\n",
       "|HeartDisease|            features|\n",
       "+------------+--------------------+\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[1,4,7,10,12,...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[3,6,7,10,12,...|\n",
       "|         0.0|(18,[0,4,7,10,12,...|\n",
       "|         1.0|(18,[0,4,8,9,12,1...|\n",
       "|         1.0|(18,[0,6,8,10,12,...|\n",
       "|         0.0|(18,[2,6,7,10,12,...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[1,4,7,9,12,1...|\n",
       "|         1.0|(18,[0,4,8,9,12,1...|\n",
       "|         0.0|(18,[2,5,7,10,12,...|\n",
       "|         0.0|(18,[1,4,7,11,12,...|\n",
       "|         1.0|(18,[0,4,7,10,12,...|\n",
       "|         1.0|(18,[0,4,8,9,12,1...|\n",
       "|         1.0|(18,[0,4,7,9,12,1...|\n",
       "|         1.0|(18,[0,4,8,11,12,...|\n",
       "|         0.0|(18,[2,4,7,10,12,...|\n",
       "|         0.0|(18,[1,4,7,10,12,...|\n",
       "+------------+--------------------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "test_prc = pipeline_trained.transform(data_test).select(target,'features')\n",
    "test_prc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111127",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Training a simple model and check its performance. This part is just for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11111128",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark2.pyspark\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(featuresCol='features', labelCol=target)\n",
    "\n",
    "logistic_trained = logistic_model.fit(train_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11111129",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+---+---+\n",
       "|HeartDisease_prediction|0.0|1.0|\n",
       "+-----------------------+---+---+\n",
       "|                    1.0| 42|321|\n",
       "|                    0.0|241| 47|\n",
       "+-----------------------+---+---+\n",
       "\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "train_predicted = logistic_trained.transform(train_prc)\n",
    "train_predicted.crosstab(target, 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11111130",
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------------------+---+---+\n",
       "|HeartDisease_prediction|0.0|1.0|\n",
       "+-----------------------+---+---+\n",
       "|                    1.0| 20|125|\n",
       "|                    0.0|100| 22|\n",
       "+-----------------------+---+---+\n",
       "\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%spark2.pyspark\n",
    "test_predicted = logistic_trained.transform(test_prc)\n",
    "test_predicted.crosstab(target, 'prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
